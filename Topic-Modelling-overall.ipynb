{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# get data file names\n",
    "path =r'D:/VisualStudioCode/twitter data/bitcoin'\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "dfs = []\n",
    "for filename in filenames:\n",
    "    dfs.append(pd.read_csv(filename))\n",
    "\n",
    "path1 =r'D:/VisualStudioCode/twitter data/ethereum'\n",
    "filenames1 = glob.glob(path1 + \"/*.csv\")\n",
    "\n",
    "for filename in filenames1:\n",
    "    dfs.append(pd.read_csv(filename))\n",
    "\n",
    "path2 =r'D:/VisualStudioCode/twitter data/litecoin'\n",
    "filenames2 = glob.glob(path2 + \"/*.csv\")\n",
    "\n",
    "for filename in filenames2:\n",
    "    dfs.append(pd.read_csv(filename))\n",
    "\n",
    "# Concatenate all data into one DataFrame\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df = pd.DataFrame(df[['tweets','likes','time']])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.tweets.str.contains('RT')]\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial cleaning of the tweets \n",
    "import re\n",
    "\n",
    "def cleanTweet(Tweet):\n",
    "    Tweet = re.sub('#[A-Za-z0-9]+',' ', Tweet)\n",
    "    Tweet = re.sub('@[A-Za-z0-9]+',' ', Tweet)\n",
    "    Tweet = re.sub('\\\\n', '', Tweet)\n",
    "    Tweet = re.sub('https?:\\/\\/\\S+',' ', Tweet)\n",
    "    Tweet = re.sub('[0-9]', ' ', Tweet)\n",
    "    return Tweet \n",
    "\n",
    "df['tweets'] = df['tweets'].apply(cleanTweet)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df['tweets'])\n",
    "df['text'] = df['tweets']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercasing\n",
    "\n",
    "import string\n",
    "df['text'] = df[\"text\"].str.lower()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: remove_punctuation(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: remove_stopwords(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing emojis\n",
    "import re\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: remove_emoji(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization with PART OF SPEECH TAGGING\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "df[\"text_lemmatized\"] = df[\"text\"].apply(lambda text: lemmatize_words(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing frequent words\n",
    "\n",
    "df1 = pd.DataFrame(df)\n",
    "\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in df1[\"text\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "        \n",
    "cnt.most_common(15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQWORDS = set([w for (w, wc) in cnt.most_common(15)])\n",
    "def remove_freqwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
    "\n",
    "df1[\"text_no_freqW\"] = df1[\"text_lemmatized\"].apply(lambda text: remove_freqwords(text))\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries for LDA\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to list\n",
    "topic = df.text_lemmatized.values.tolist()\n",
    "\n",
    "topic_nofreq = df1.text_no_freqW.values.tolist()\n",
    "\n",
    "print(topic[:1])\n",
    "\n",
    "print(topic_nofreq[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing\n",
    "def tokenize(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True)) \n",
    "\n",
    "topic_words = list(tokenize(topic))\n",
    "topic_words_nof = list(tokenize(topic_nofreq))\n",
    "\n",
    "print(topic_words[:1])\n",
    "print(topic_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram models\n",
    "bigram = gensim.models.Phrases(topic_words, min_count=5, threshold=100) \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "\n",
    "#Build the bigram models NOFREQ\n",
    "bigram1 = gensim.models.Phrases(topic_words_nof, min_count=5, threshold=100)\n",
    "bigram_mod1 = gensim.models.phrases.Phraser(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making bigrams\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_bigrams1(texts):\n",
    "    return [bigram_mod1[doc] for doc in texts]\n",
    "\n",
    "topic_words = make_bigrams(topic_words)\n",
    "\n",
    "topic_words_nof = make_bigrams1(topic_words_nof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(topic_words)\n",
    "\n",
    "# Create Corpus\n",
    "texts = topic_words\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOFREQ\n",
    "# Create Dictionary\n",
    "id2word1 = corpora.Dictionary(topic_words_nof)\n",
    "\n",
    "# Create Corpus\n",
    "texts1 = topic_words_nof\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus1 = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus1[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=15, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=15,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOFREQ\n",
    "#Build LDA model\n",
    "lda_model1 = gensim.models.ldamodel.LdaModel(corpus=corpus1,\n",
    "                                           id2word=id2word1,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=20,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model1.print_topics())\n",
    "doc_lda1 = lda_model1[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=topic_words, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model1.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda1 = CoherenceModel(model=lda_model1, texts=topic_words_nof, dictionary=id2word1, coherence='c_v')\n",
    "coherence_lda1 = coherence_model_lda1.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.save('topic.model')\n",
    "lda_model1.save('topic.model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the topics\n",
    "pyLDAvis.enable_notebook(local=True)\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the topics NOFREQ\n",
    "pyLDAvis.enable_notebook()\n",
    "vis1 = pyLDAvis.gensim_models.prepare(lda_model1, corpus1, id2word1)\n",
    "vis1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing\n",
    "\n",
    "topic = pd.DataFrame(df['text_lemmatized'])\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens \n",
    "\n",
    "topic['text'] = topic['text_lemmatized'].apply(lambda x: tokenize(x))\n",
    "\n",
    "topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim #the library for Topic modelling\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim import corpora, models\n",
    "import pyLDAvis.gensim_models #LDA visualization library\n",
    "\n",
    "#create dictionary\n",
    "dictionary = corpora.Dictionary(topic['text'])\n",
    "\n",
    "#create document term matrix\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in topic['text'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "num_topics=10\n",
    "%time ldamodel = lda(doc_term_matrix,num_topics=num_topics,id2word=dictionary,passes=50,minimum_probability=0)\n",
    "\n",
    "lda_display = pyLDAvis.gensim_models.prepare(ldamodel, doc_term_matrix, dictionary, sort_topics=False, mds='mmds')\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing\n",
    "\n",
    "topic_nofreq = pd.DataFrame(df['text_no_freqW'])\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens \n",
    "\n",
    "topic_nofreq['text'] = topic_nofreq['text_no_freqW'].apply(lambda x: tokenize(x))\n",
    "\n",
    "topic_nofreq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim #the library for Topic modelling\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim import corpora, models\n",
    "import pyLDAvis.gensim_models #LDA visualization library\n",
    "\n",
    "#create dictionary\n",
    "dictionary = corpora.Dictionary(topic_nofreq['text'])\n",
    "\n",
    "#create document term matrix\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in topic_nofreq['text'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "num_topics=10\n",
    "%time ldamodel = lda(doc_term_matrix,num_topics=num_topics,id2word=dictionary,passes=50,minimum_probability=0)\n",
    "\n",
    "lda_display = pyLDAvis.gensim_models.prepare(ldamodel, doc_term_matrix, dictionary, sort_topics=False, mds='mmds')\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc5f70855ac006f3de45a3cc3b9e7d8d53845e50458809cb162b0174266dec97"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
